{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45cd333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting firecrawl\n",
      "  Downloading firecrawl-2.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from firecrawl)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting python-dotenv (from firecrawl)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting websockets (from firecrawl)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\mts19\\appdata\\roaming\\python\\python313\\site-packages (from firecrawl) (1.6.0)\n",
      "Collecting pydantic (from firecrawl)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting aiohttp (from firecrawl)\n",
      "  Downloading aiohttp-3.11.18-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->firecrawl)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->firecrawl)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->firecrawl)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->firecrawl)\n",
      "  Downloading frozenlist-1.6.0-cp313-cp313-win_amd64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->firecrawl)\n",
      "  Downloading multidict-6.4.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->firecrawl)\n",
      "  Downloading propcache-0.3.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->firecrawl)\n",
      "  Downloading yarl-1.20.0-cp313-cp313-win_amd64.whl.metadata (74 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->firecrawl)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic->firecrawl)\n",
      "  Downloading pydantic_core-2.33.1-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic->firecrawl)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->firecrawl)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->firecrawl)\n",
      "  Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->firecrawl)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->firecrawl)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->firecrawl)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading firecrawl-2.3.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiohttp-3.11.18-cp313-cp313-win_amd64.whl (437 kB)\n",
      "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 514.7 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 639.4 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 639.4 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 639.4 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 589.3 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 589.3 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 572.5 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 572.5 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 572.5 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.6/2.0 MB 559.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 559.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 588.9 kB/s eta 0:00:00\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl (176 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Downloading frozenlist-1.6.0-cp313-cp313-win_amd64.whl (119 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading multidict-6.4.3-cp313-cp313-win_amd64.whl (38 kB)\n",
      "Downloading propcache-0.3.1-cp313-cp313-win_amd64.whl (44 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading yarl-1.20.0-cp313-cp313-win_amd64.whl (92 kB)\n",
      "Installing collected packages: websockets, urllib3, typing-extensions, python-dotenv, propcache, multidict, idna, frozenlist, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspection, requests, pydantic-core, aiosignal, pydantic, aiohttp, firecrawl\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 attrs-25.3.0 certifi-2025.1.31 charset-normalizer-3.4.1 firecrawl-2.3.0 frozenlist-1.6.0 idna-3.10 multidict-6.4.3 propcache-0.3.1 pydantic-2.11.3 pydantic-core-2.33.1 python-dotenv-1.1.0 requests-2.32.3 typing-extensions-4.13.2 typing-inspection-0.4.0 urllib3-2.4.0 websockets-15.0.1 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script websockets.exe is installed in 'c:\\Users\\mts19\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\mts19\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\mts19\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# install firecrawl if not installed \n",
    "%pip install firecrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbabfb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRECRAWL_API_KEY = 'fc-ce38ce36fb854e9197959647181a8163'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping year: 2012\n",
      "Found 0 articles for year 2012.\n",
      "Scraping year: 2013\n",
      "Found 0 articles for year 2012.\n",
      "Scraping year: 2013\n",
      "Found 0 articles for year 2013.\n",
      "Scraping year: 2014\n",
      "Found 0 articles for year 2013.\n",
      "Scraping year: 2014\n",
      "Found 0 articles for year 2014.\n",
      "Scraping year: 2015\n",
      "Found 0 articles for year 2014.\n",
      "Scraping year: 2015\n",
      "Found 0 articles for year 2015.\n",
      "Scraping year: 2016\n",
      "Found 0 articles for year 2015.\n",
      "Scraping year: 2016\n",
      "Found 0 articles for year 2016.\n",
      "Scraping year: 2017\n",
      "Found 0 articles for year 2016.\n",
      "Scraping year: 2017\n",
      "Found 0 articles for year 2017.\n",
      "Scraping year: 2018\n",
      "Found 0 articles for year 2017.\n",
      "Scraping year: 2018\n",
      "Found 0 articles for year 2018.\n",
      "Scraping year: 2019\n",
      "Found 0 articles for year 2018.\n",
      "Scraping year: 2019\n",
      "Found 0 articles for year 2019.\n",
      "Scraping year: 2020\n",
      "Found 0 articles for year 2019.\n",
      "Scraping year: 2020\n",
      "Found 0 articles for year 2020.\n",
      "Scraping year: 2021\n",
      "Found 0 articles for year 2020.\n",
      "Scraping year: 2021\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "Request Timeout: Failed to scrape URL as the request timed out. Request timed out - No additional error details provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2012\u001b[39m, \u001b[32m2026\u001b[39m):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mScraping year: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     scrape_result = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscrape_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m?year=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmarkdown\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhtml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Extract articles from the scrape result\u001b[39;00m\n\u001b[32m     25\u001b[39m     articles = scrape_result.articles \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(scrape_result, \u001b[33m'\u001b[39m\u001b[33marticles\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mts19\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\firecrawl\\firecrawl.py:559\u001b[39m, in \u001b[36mFirecrawlApp.scrape_url\u001b[39m\u001b[34m(self, url, formats, include_tags, exclude_tags, only_main_content, wait_for, timeout, location, mobile, skip_tls_verification, remove_base64_images, block_ads, proxy, extract, json_options, actions, **kwargs)\u001b[39m\n\u001b[32m    557\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mFailed to parse Firecrawl response as JSON.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscrape URL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mts19\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\firecrawl\\firecrawl.py:2175\u001b[39m, in \u001b[36mFirecrawlApp._handle_error\u001b[39m\u001b[34m(self, response, action)\u001b[39m\n\u001b[32m   2172\u001b[39m message = \u001b[38;5;28mself\u001b[39m._get_error_message(response.status_code, action, error_message, error_details)\n\u001b[32m   2174\u001b[39m \u001b[38;5;66;03m# Raise an HTTPError with the custom message and attach the response\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2175\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m requests.exceptions.HTTPError(message, response=response)\n",
      "\u001b[31mHTTPError\u001b[39m: Request Timeout: Failed to scrape URL as the request timed out. Request timed out - No additional error details provided."
     ]
    }
   ],
   "source": [
    "from firecrawl import FirecrawlApp\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize the FirecrawlApp with your API key\n",
    "app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "\n",
    "# Define the base URL\n",
    "base_url = \"https://english.alarabiya.net/News/middle-east\"\n",
    "\n",
    "# Open a CSV file to save the results\n",
    "with open('arab_dataset.csv', 'w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow(['Title', 'Content', 'Location', 'Time'])\n",
    "\n",
    "    # Loop through the years from 2012 to 2025\n",
    "    for year in range(2012, 2026):\n",
    "        logging.info(f'Scraping year: {year}')\n",
    "        try:\n",
    "            scrape_result = app.scrape_url(\n",
    "                url=f'{base_url}?year={year}',\n",
    "                formats=['markdown', 'html']\n",
    "            )\n",
    "\n",
    "            # Check if scrape_result is valid\n",
    "            if not scrape_result or not hasattr(scrape_result, 'articles'):\n",
    "                logging.warning(f'No valid response for year {year}. Response: {scrape_result}')\n",
    "                continue\n",
    "\n",
    "            # Extract articles from the scrape result\n",
    "            articles = scrape_result.articles\n",
    "            logging.info(f'Found {len(articles)} articles for year {year}.')\n",
    "\n",
    "            # Write the data\n",
    "            for article in articles:\n",
    "                title = article.get('title', '').replace(',', ' ')\n",
    "                content = article.get('content', '').replace(',', ' ')\n",
    "                location = article.get('location', 'Unknown').replace(',', ' ')\n",
    "                time = article.get('date', '')\n",
    "                writer.writerow([title, content, location, time])\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error scraping year {year}: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
